{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/agarwalsiddhant10/xai_isact_2021/blob/main/XAI_ICAST_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ies7TaS9i4cX"
   },
   "source": [
    "## XAI Workshop\n",
    "\n",
    "This is a tutorial on the commonly used XAI techniques for Image Classification.\n",
    "\n",
    "We use Python Programming language for the tutorial so we expect you to have sufficient experience with Python. We also expect you to have a good profiency in using deep learning frameworks, we will be using ```pytorch``` for all of the coding tasks below.\n",
    "\n",
    "You will be asked to code 3 algorithms (defined as classes) namely RISE, CAM and GradCAM as discussed in the lecture. We have already written snippets to help you. The saliency maps that you generate should have values between $0$ and $1$ with $0$ being the least important region and $1$ being the most. For RISE, the shape of the map you return will be ```(No. of class, H, W)``` but for CAM and GradCAM, since you need to mention the class while computing the saliency map, the shape will naturally be ```(H, W)```. Also note that the maps returned must by numpy arrays rather than torch tensors. You need to write your code between ```###START CODE HERE###``` and ```###END CODE HERE###``` markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a module for performing auxilliary functions like sampling an image, viewing the saliency map etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/agarwalsiddhant10/xai_isact_2021.git\n",
    "import xai_isact_2021.xai_utils as xai_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a random image\n",
    "random_image_path = xai_utils.sample_random_image()\n",
    "image_tensor = xai_utils.read_tensor(random_image_path).float().cuda()\n",
    "\n",
    "# Creating an instance of a pretrained resnet50 model\n",
    "model = torchvision.models.resnet50(True)\n",
    "model.eval().cuda()\n",
    "\n",
    "# View the predictions on the sampled image\n",
    "logits = model(image_tensor)\n",
    "class_to_explain = np.argmax(logits.cpu().detach().numpy())\n",
    "xai_utils.tensor_imshow(image_tensor, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Randomized Importance Sampling for Explanations\n",
    "[[Paper](https://arxiv.org/pdf/1806.07421.pdf)]\n",
    "\n",
    "The following image will briefly explain the algorithm.\n",
    "![RISE](https://raw.githubusercontent.com/agarwalsiddhant10/xai_isact_2021/main/imgs/rise.png)\n",
    "\n",
    "The first step is to generate random masks. RISE generates random masks of size $(s, s)$ and then linearly upsamples it and crops it. We have already written the code for that.\n",
    "\n",
    "The next step is the apply these masks on the input and use the predicted class scores to linearly combine these masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RISE(nn.Module):\n",
    "    def __init__(self, model, input_size):\n",
    "        super(RISE, self).__init__()\n",
    "        self.model = model\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def generate_masks(self, N, s, p1):\n",
    "        '''\n",
    "        Args:\n",
    "                N (int): Number of masks\n",
    "                s (int): Size of the random mask (this will later be upscaled to the input size)\n",
    "                p1 (float): probability to unmask\n",
    "                \n",
    "        Note: We have written this function. Do have a look at the different operations for \n",
    "        generating the random masks.\n",
    "        '''\n",
    "        \n",
    "        # Computing the size of the random mask\n",
    "        cell_size = np.ceil(np.array(self.input_size) / s)\n",
    "        up_size = (s + 1) * cell_size\n",
    "\n",
    "        # Creating the N drandom mask \n",
    "        grid = np.random.rand(N, s, s) < p1\n",
    "        grid = grid.astype('float32')\n",
    "\n",
    "        # Create an empty numpy array to store the upsampled masks\n",
    "        self.masks = np.empty((N, *self.input_size))\n",
    "\n",
    "        # Iterate over the masks\n",
    "        for i in tqdm(range(N), desc='Generating filters'):\n",
    "            # Random shifts\n",
    "            x = np.random.randint(0, cell_size[0])\n",
    "            y = np.random.randint(0, cell_size[1])\n",
    "            # Linear upsampling and cropping\n",
    "            self.masks[i, :, :] = resize(grid[i], up_size, order=1, mode='reflect',\n",
    "                                         anti_aliasing=False)[x:x + self.input_size[0], y:y + self.input_size[1]]\n",
    "            \n",
    "        # Convert the masks to torch tensor and store it\n",
    "        self.masks = self.masks.reshape(-1, 1, *self.input_size)\n",
    "        self.masks = torch.from_numpy(self.masks).float()\n",
    "        self.masks = self.masks.cuda()\n",
    "        self.N = N\n",
    "        self.p1 = p1\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            ###START CODE HERE###\n",
    "\n",
    "            # Apply array of filters to the image\n",
    "            \n",
    "            # Compute the predictions for the masked images\n",
    "            \n",
    "            # Number of classes\n",
    "            \n",
    "            # Linearly combine the masks using the prediction scores as weights\n",
    "\n",
    "            # Final processing. Remember, the saliency maps must be shape (CL, H, W)\n",
    "\n",
    "            ###END CODE HERE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the RISE class is defined, it can be easily used to compute the saliency map as shown in the following code. We will generate the saliency map to explain the top predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = RISE(model, (224, 224))\n",
    "explainer.generate_masks(1000, 7, 0.1)\n",
    "salmaps = explainer(image_tensor)\n",
    "salmaps = salmaps.cpu().numpy()\n",
    "\n",
    "xai_utils.show_saliency(image_tensor, salmaps[class_to_explain])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Class Activation Mapping\n",
    "[[Paper](http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf)]\n",
    "\n",
    "CAM, unlike RISE, is a white box explanation technique. It uses the weights and the activations of the network to compute the importance map.\n",
    "\n",
    "![CAM](https://raw.githubusercontent.com/agarwalsiddhant10/xai_isact_2021/main/imgs/cam.png)\n",
    "\n",
    "As shown in the image, CAM uses the activations of the last feature map and weighs them using the weights for the linear layer that follows it (and is used for classification). As you can see, CAM can be ideally applied on a very selective range of architectures. This is the disadvantage of white box techniques.\n",
    "\n",
    "You will need to use a hook to save the activations during the forward pass. Hooks are functions that are executed during either forward or backward pass. Naturally, there are two kinds of hooks, one for each type of pass. For CAM, you will require only forward hook. In pytorch, you can register a forward hook to a tensor or network parameter using the function ```<net_param>.register_forward_hook(<hook>)```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAM(nn.Module):\n",
    "    def __init__(self, model, feature_name):\n",
    "        super(CAM, self).__init__()\n",
    "        self.model = model\n",
    "        self.feature_name = feature_name\n",
    "\n",
    "        ###START CODE HERE###\n",
    "        # Create and register a forward hook for saving the activatiions of the final feature map\n",
    "\n",
    "        # Extract the weights of the linear layer or the final predictor\n",
    "        \n",
    "        ###END CODE HERE###\n",
    "\n",
    "    \n",
    "    def forward(self, input_tensor, class_idx):\n",
    "        ###START CODE HERE###\n",
    "\n",
    "        # Forward Pass\n",
    "\n",
    "        # Construct CAM by multiplying weights and activations\n",
    "        \n",
    "        #Upsample the cam to the image size\n",
    "        \n",
    "        ###END CODE HERE###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following snippet to check if your class works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = CAM(model, 'layer4')\n",
    "cam = explainer(image_tensor, class_to_explain)\n",
    "xai_utils.show_saliency(image_tensor, cam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAM opens up a new class of techniques for generating saliency maps, CAM being the simplest of the lot. GradCAM is an extention to CAM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GradCAM\n",
    "[[Paper](https://arxiv.org/pdf/1610.02391.pdf)]\n",
    "\n",
    "GradCAM is similar to CAM but here instead of multiplying the activations with the model weights, we use gradients when the outputs of the explained class is backpropagated through the model.\n",
    "\n",
    "Clearly the class will look very similar to the CAM class. Here since we are using gradients as well, you need to use both forward and backward hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM(nn.Module):\n",
    "    def __init__(self, model, feature_name):\n",
    "        super(GradCAM, self).__init__()\n",
    "        self.model = model\n",
    "        self.feature_name = feature_name\n",
    "\n",
    "        ###START CODE HERE####\n",
    "        # Create the forward and backward hooks\n",
    "\n",
    "        ###END CODE HERE###\n",
    "\n",
    "    \n",
    "    def forward(self, input_tensor, class_idx):\n",
    "        ###START CODE HERE###\n",
    "\n",
    "        # Forward pass\n",
    "\n",
    "        # Backward pass through the outputs\n",
    "\n",
    "        # Multiply the gradients and activations\n",
    "        \n",
    "        # Apply ReLU over the obtained product\n",
    "        \n",
    "        # Upsample the gradcam map to the image size\n",
    "    \n",
    "        ###END CODE HERE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the snippet below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = GradCAM(model, 'layer4')\n",
    "gradcam = explainer(image_tensor, class_to_explain)\n",
    "xai_utils.show_saliency(image_tensor, gradcam)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPPRaA3LLsmmCUgxdGp+EnH",
   "include_colab_link": true,
   "name": "XAI_ICAST_2021.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
